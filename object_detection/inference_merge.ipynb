{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nW4BN-FEHAn"
   },
   "source": [
    "# Set model path (swin_crop, yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model_path = \"./runs/train/fish_yolov5x_pre_640_G075/weights/best.pt\"\n",
    "swin_crop_path = \"../checkpoint/swin_crop/checkpoint.pth.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set stg1, stg2 image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_stg1 = \"../../../DATA/fish/test_stg1/\"\n",
    "data_path_stg2 = \"../../../DATA/fish/test_stg2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set swin_full_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_csv_path = \"eval_swin_transformer_large_384_2021-12-20-15-44-08-add01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4M1nNaTRgQm",
    "outputId": "09d162b4-f6ea-4768-bac4-f9e6df2f0b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu102 True\n"
     ]
    }
   ],
   "source": [
    "# Import your package and check the version\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "\n",
    "# You must import the below 5 packages \n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "from utils.general import check_file, check_img_size, check_imshow, non_max_suppression, scale_coords, xyxy2xywh\n",
    "from utils.datasets import LoadImages\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTs4phq2W8zG",
    "outputId": "c001ab7e-08ee-4405-decb-987b7ef2329a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ ba6da5f torch 1.9.0+cu102 CUDA:0 (NVIDIA TITAN RTX, 24220MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 444 layers, 86220517 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = ''\n",
    "device = select_device(device)\n",
    "model = DetectMultiBackend(yolo_model_path, device=device, dnn=False)\n",
    "stride, names, pt, jit, onnx = model.stride, model.names, model.pt, model.jit, model.onnx\n",
    "if pt and device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, 640, 480).to(device).type_as(next(model.model.parameters())))  # warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_classification = torch.load(swin_crop_path).eval().to(device)\n",
    "smax = nn.Softmax(dim=1)\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def classification(img):\n",
    "    with torch.no_grad():\n",
    "        img = transform(img)\n",
    "        avg_B = torch.mean(img[0])\n",
    "        avg_G = torch.mean(img[1])\n",
    "        avg_R = torch.mean(img[2])\n",
    "        if (avg_G - 0.05 > avg_B and avg_G - 0.05 > avg_R):\n",
    "            img[1] = img[1] * 0.75\n",
    "        data = img.unsqueeze(0).to(device)\n",
    "        output = model_classification(data)\n",
    "        output = smax(output).detach().cpu().numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVr_Mls8NBe6",
    "outputId": "094b9b68-e14b-4ffa-ffe6-2fcc87989284"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:41<00:00, 23.97it/s]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 9405/12153 [05:00<01:30, 30.32it/s]"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "dataset_stg1 = LoadImages(data_path_stg1, img_size=(640,480), stride=stride, auto=pt and not jit)\n",
    "dataset_stg2 = LoadImages(data_path_stg2, img_size=(640,480), stride=stride, auto=pt and not jit)\n",
    "\n",
    "conf_thres=0.25\n",
    "iou_thres=0.45\n",
    "max_det=100\n",
    "agnostic_nms=False\n",
    "classes=None\n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "thickness = 2\n",
    "\n",
    "# for each test image\n",
    "def inference_to_csv(dataset, writer, stg):\n",
    "    for img_path, img, im0s, vid_cap, s in tqdm(dataset):\n",
    "        img_name = os.path.basename(img_path)\n",
    "        if (stg == 2):\n",
    "            image_id = int(img_name[6:-4])\n",
    "        elif (stg == 1):\n",
    "            image_id = int(img_name[4:-4])\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        img /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        avg_B = torch.mean(img[0])\n",
    "        avg_G = torch.mean(img[1])\n",
    "        avg_R = torch.mean(img[2])\n",
    "        if (avg_G - 0.05 > avg_B and avg_G - 0.05 > avg_R):\n",
    "            img[1] = img[1] * 0.75\n",
    "        if len(img.shape) == 3:\n",
    "            img = img[None]  # expand for batch dim\n",
    "        pred = model(img)\n",
    "        pred_max = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "        det = pred_max[0]\n",
    "        pred = pred_max\n",
    "        gn = torch.tensor(im0s.shape)[[1, 0, 1, 0]]\n",
    "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n",
    "        \n",
    "        pre_value = [0,0,0,0,0,0,0,0]\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "            w = int(xywh[2] * im0s.shape[1])\n",
    "            h = int(xywh[3]* im0s.shape[0])\n",
    "            x = int((xywh[0] * im0s.shape[1]) - (w/2))\n",
    "            y = int((xywh[1] * im0s.shape[0]) - (h/2))\n",
    "            crop_img = im0s[y:y+h, x:x+w]\n",
    "            classification_pre = classification(crop_img)[0]\n",
    "            pre_value[0] = max(float(classification_pre[0]), pre_value[0])\n",
    "            pre_value[1] = max(float(classification_pre[1]), pre_value[1])\n",
    "            pre_value[2] = max(float(classification_pre[2]), pre_value[2])\n",
    "            pre_value[3] = max(float(classification_pre[3]), pre_value[3])\n",
    "            pre_value[5] = max(float(classification_pre[4]), pre_value[5])\n",
    "            pre_value[6] = max(float(classification_pre[5]), pre_value[6])\n",
    "            pre_value[7] = max(float(classification_pre[6]), pre_value[7])\n",
    "        for i in range(len(pre_value)):\n",
    "            if i == 1 or i == 7:\n",
    "                pre_value[i]+=0.05\n",
    "            else:\n",
    "                pre_value[i]+=0.01\n",
    "        if (len(det) == 0):\n",
    "            pre_value[4] = float(1)\n",
    "        if (stg == 2):\n",
    "            img_name = \"test_stg2/\" + img_name\n",
    "        row = [img_name] + pre_value\n",
    "        writer.writerow(row)\n",
    "   \n",
    "\n",
    "with open(\"answer.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"image\", \"ALB\", \"BET\", \"DOL\", \"LAG\", \"NoF\", \"OTHER\", \"SHARK\", \"YFT\"])\n",
    "    inference_to_csv(dataset_stg1, writer, 1)\n",
    "    inference_to_csv(dataset_stg2, writer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import csv\n",
    "yolo_csv = pd.read_csv(\"answer.csv\")\n",
    "swin_csv = pd.read_csv(swin_csv_path)\n",
    "\n",
    "with open('answer_merge.csv', \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"image\", \"ALB\", \"BET\", \"DOL\", \"LAG\", \"NoF\", \"OTHER\", \"SHARK\", \"YFT\"])\n",
    "    for yolo, swin in zip(yolo_csv.iterrows(), swin_csv.iterrows()):\n",
    "        if(yolo[1]['NoF'] > 0.4):\n",
    "            writer.writerow(swin[1])\n",
    "        else:\n",
    "            writer.writerow(yolo[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
